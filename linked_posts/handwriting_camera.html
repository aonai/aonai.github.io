<html >
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="css/posts.css">
    <head>
        <title>Handwriting Camera</title>
    </head>

    <nav class="navbar">
        <a href="/.."><i class="fa fa-home" ></i></a>
    </nav>  

    <nav class="navbar-menu">
        <a href="#page-top">Back to Top</a>
        <a href="#overview">Overview</a>
        <a href="#machine-learning">Machine Learning</a>
        <a href="#computer-vision">Computer Vision</a>
        <a href="#integration">Integration</a>
        <a href="#related-links">Related Links</a>
    </nav> 


    <body id="page-top" class="index">
        <h1 class='title'>Handwriting Camera</h1>

        <hr class="star-primary">
        <div class="under-title">
            <div class="container">
                <div clsas="row">
                    <div class="col-lg-2 ">
                        <p>
                            Skills:  
                            <br>
                            Time:  
                        </p>
                    </div>
                    <div class="col-lg-10 ">
                        <p>
                            Pytorch, OpenCV, RealSense Depth Camera
                            <br>
                            Winter, 2020-2021  
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <iframe width="750" height="422" src="https://www.youtube.com/embed/9Fl5xeTdH-4" frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
        </iframe>

        <hr class="para-break" data-content="  Overview  " id="overview">
        <p>
            This project is my independent study of the winter quarter. The goal of this project is to classify letters 
            written in front of a camera. The code can be run with either an Intel&reg;RealSense&trade; Depth Camera or
            an integrated webcam. The code will track the pen by using the Haar Cascade object detection algorithm or filtering 
            specified colors and display the trajectory on the screen. When the user is done writing, the letter prediction is
            output by a pre-trained PyTorch model. The project can be separated into three main tasks: 
            <a href="#machine-learning">machine learning</a>, 
            <a href="#computer-vision">computer vision</a>, and
            <a href="#integration">integration</a>. 
        </p>
        
        <hr class="para-break" data-content="  Machine Learning  " id="machine-learning">
        <p>
            This project trains a model using 
            <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch </a>
            and the
            <a href="https://arxiv.org/abs/1702.05373" target="_blank" rel="noopener noreferrer">EMNIST</a> letter dataset. 
            The classes are alphabets 
            from A to Z. Due to this structure, lowercase letters will also be classified as uppercases.
            When loading the dataset, all EMNIST images are transformed into PyTorch tensor objects and normalized. 
            Both training and testing sets are shuffled. The batch size of the loaders is the same as the size of classes,
            which is 26. One example of a batch of images with their ground truth is shown below. 
        </p>

        </p>
        <img src="../img/portfolio/ws-example-batch.jpg" style="width:40%">

        <p>
            The neural network used during training has two 2D convolution layers and three hidden layers.
            In the beginning, a 2d max pooling is applied to the inputs. All layers are followed by rectified linear units (ReLU) 
            as their activation functions.
        </p>
        <img src="../img/portfolio/ws-nn-architecture.png" style="width:60%">
        
        <p>
            The overall accuracy of the trained model on the EMNIST letter dataset is around 93.3%. 
            Accuracies on each class for most classes are above 90%. The image below shows the accuracies of 
            the model for each class.  
        </p>
        <img src="../img/portfolio/ws-test-acc.jpg" style="width:60%">
        
        <p>
            As seen from the image above, the exceptions are G, I, and L. It is reasonable that I and L 
            are easily confused, given that lower case i and l are very similar. As for G, this is likely to be
            confused with lowercase Q. The images below are lowercase letters I, L, Q, G from left to right. In 
            general, the model
            should correctly classify around 70% of these letters in the testing set.
        </p>
        <div class="container">
            <div clsas="row">
                <div class="col-lg-2 ">
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_i.png" style="width:80%" >  
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_l.png" style="width:80%" >  
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_q.png" style="width:80%" >  
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_g.png" style="width:80%" >  
                </div>
            </div>
        </div>

        <p>
            To test user inputs, at least three images are stored for each label. The overall accuracy is 96.3%. 
            Accuracies for each class are shown below. Notice that the test set has only 178 images and is all 
            written under the same condition, so the accuracies may be biased. 
        </p>
        <img src="../img/portfolio/ws-user-input-test.jpg"  style="width:60%" > 
        
        <p>
            Most classes can be classified. The images that are misclassified are likely due to 
            the shaking behavior while writing. Some of the lines are not correctly bounded or closed. 
            One example is the D below that is classified as P. 
        </p>
        <img src="../img/portfolio/ws-user-d.jpg"  style="width:10%" > 
        
        <hr class="para-break" data-content="  Computer Vision  " id="computer-vision">
        <p>
            At the start of writing this project, tracking a pen is configured by filtering out a specific color range. 
            Here, a red-colored pen is used. To make detecting the color range easier, it also uses 
            <a href="https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html" target="_blank" rel="noopener noreferrer">OpenCV background subtraction </a>
            function to overlay a grey mask on the steady background. 
            However, some false positives persist. To filter out the noises, the position of the pen on screen is 
            defined as the average of positions in five consecutive frames. As a result, tracking the pen is a little slow. 
            This sometimes causes the strokes to be unsmooth. 
        </p>
        <br>
        <p>
            To improve performance, the project has an option to use 
            <a href="https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html" target="_blank" rel="noopener noreferrer"> Haar Cascade </a>
            train and detect functions provided by OpenCV to track a pen. 
            The pen used for tracking is bought from  
            <a href="https://yoobi.com/collections/pens/products/scented-gel-pens-6-pack-fruit" target="_blank" rel="noopener noreferrer">yoobi</a>.
            To ensure that the pen is easily classifiable, a distinctive portion of the pen is chosen as the object
            to detect. The tracked portion is bounded by a red box in the image shown on the right below. 
        </p>
        <div class="container">
            <div clsas="row">
                <div class="col-lg-2 ">
                </div>
                <div class="col-lg-3 ">
                    <img src="https://cdn.shopify.com/s/files/1/0445/6817/products/TARGET_INLINE_MARCH_2020-4410-CROPPED_1000x.jpg?v=1582241292" 
                    style="width:100%" >  
                </div>
                <div class="col-lg-5 ">
                    <img src="../img/portfolio/ws-tracked-pen.png" style="width:40%" >  
                </div>
            </div>
        </div>

        <p>
            The Haar Cascade model used in the classifier can correctly detect the green pen in front of a depth camera; however, it will occasionally misclassify 
            other objects (such as fingers or shadows of similar shape) as the pen. To solve this problem without having to 
            retrain the model with more images, the code assumes that only one object is detected at each frame and that 
            10 neighboring objects need to be detected before defining it as the pen. The code also bounds the size of detection between 30x30 and 80x80 
            pixels. With these conditions, small noises are neglected, but it also has the risk of not recognizing 
            the pen due to the large detection threshold. This problem is solved by increasing the frame rate. 
        </p>

        <p>
            <br>
            When processing color frames from a depth camera at a frame rate of 60 fps,
            the performance is better than running at 30 fps, but more false detections are found in this case. As a result, 
            the track function adds in another condition that if detection is around twice width or 
            twice height pixels away from its previous detection, then this detection is considered incorrect. 
            The images below show computer vision tracking a pen using color filtering at 60fps, Haar Cascade model 
            at 30 fps, and 60 fps respectively.
        </p>
        
        <div class="container">
            <div clsas="row">
                <div class="col-lg-1 "></div>
                <div class="col-lg-3 ">
                    <img src="../img/portfolio/ws-track-color.gif" style="width:100%">  
                </div>
                <div class="col-lg-4 ">
                    <img src="../img/portfolio/ws-30fps.gif" style="width:70%" >  
                </div>
                <div class="col-lg-3">
                    <img src="../img/portfolio/ws-60fps.gif" style="width:100%" >  
                </div>
            </div>
        </div>

        <hr class="para-break" data-content="  Integration  " id="integration">
        <p>
            This project consists of two main classes: Tracker and Classifier. 
            Tracker is responsible for tracking a pen in front of a depth camera, and Classifier
            is responsible for classifying user inputs.
        </p>
        <img src="../img/portfolio/wc-simple-workflow.png" style="width:40%" >  
        <p>
            To allow proper linking between different tasks, both Tracker and Classifier need to have access to 
            the folder <i>user_images</i>, where writing images are stored. When the user is done with writing, Tracker first crops 
            the screen based on where the writing is. Then, it stores the cropped frame as an image into <i>user_images</i> and calls
            function classify from Classifier. This function takes the image file name as an input so that when 
            loading the data using <i>UserImageDataSet</i> loader, the correct image is extracted. When loading user inputs, images 
            are resized to 28x28 px to makes sure that writings of different sizes are also correctly processed. 
            Other transforms are the same as in training. Next, the transformed data is input into the pre-trained PyTorch model 
            to predict its class. Finally, the prediction is sent back to Tracker to have it displayed on the screen.
        </p>
        <br>
        <p>
            When Tracker is tracking the pen, it is also waiting for user keyboard inputs. Options of these inputs include clear board, 
            clear predictions, erase or write, save an image to classify or to the dataset, and redo false predictions. 
            Other than the <i>UserImageDataSet</i>, the project also provides <i>UserKnownImageDataSet</i> for loading stored and labeled images. 
            To use customized PyTorch model or Haar Cascade model, the project provides helper classes and functions for training and visualizing
            the performance. More details can be found on the project GitHub page.
        </p>

        <hr class="para-break" data-content="  Related Links  " id="related-links">
        <p>
            <i class="fa fa-github"></i>
            <a class="i_link" href="https://github.com/aonai/winter_project_handwriting_camera" target="_blank" rel="noopener noreferrer"> Source Code</a>
            <br>
            
            <i class="fa fa-file-archive-o"></i>
            <a class="i_link" href="https://drive.google.com/file/d/1Rhdzq3cQDivl3OSlLLYgq3ii3eXRylLw/view?usp=sharing" target="_blank" rel="noopener noreferrer"> Testing Dataset </a>
        </p>
    </body>
 

</html>  
