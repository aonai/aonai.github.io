<html >
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="css/posts.css">
    <head>
        <title>Handwriting Camera</title>
    </head>

    <nav class="navbar">
        <a href="/.."><i class="fa fa-home" ></i></a>
    </nav>  

    <nav class="navbar-menu">
        <a href="#page-top">Back to Top</a>
        <a href="#overview">Overview</a>
        <a href="#machine-learning">Machine Learning</a>
        <a href="#computer-vision">Computer Vision</a>
        <a href="#integration">Integration</a>
        <a href="#related-links">Related Links</a>
    </nav> 


    <body id="page-top" class="index">
        <h1 class='title'>Handwriting Camera</h1>

        <hr class="star-primary">
        <div class="under-title">
            <div class="container">
                <div clsas="row">
                    <div class="col-lg-2 ">
                        <p>
                            Skills:  
                            <br>
                            Time:  
                        </p>
                    </div>
                    <div class="col-lg-10 ">
                        <p>
                            Pytorch, OpenCV, RealSense Depth Camera
                            <br>
                            Winter, 2020-2021  
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <iframe width="750" height="422" src="https://www.youtube.com/embed/9Fl5xeTdH-4" frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
        allowfullscreen>
        </iframe>

        <hr class="para-break" data-content="  Overview  " id="overview">
        <p>
            This project is my independent study of the winter quarter. The goal of this project is to classify letters 
            written in front of a camera. The code can be run with either an Intel&reg;RealSense&trade; Depth Camera or
            an integrated webcam. The code will track pen by using Haar Cascade object detection algorithm or filtering 
            specified colors and display the trajectory on the screen. When user is done writing, the letter prediction is
            output by a pre-trained PyTorch model.  The project can be separated into three main tasks: 
            <a href="#machine-learning">machine learning</a>, 
            <a href="#computer-vision">computer vision</a>, and
            <a href="#integration">integration</a>. 
        </p>
        
        <hr class="para-break" data-content="  Machine Learning  " id="machine-learning">
        <p>
            This project trains a model using 
            <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch </a>
            and 
            <a href="https://arxiv.org/abs/1702.05373" target="_blank" rel="noopener noreferrer">EMNIST letter dataset</a>
            
            . The classes are alphabets 
            from A to Z. Due to this structure; lowercase letters will also be classified as uppercases.
            When loading the dataset, all EMNIST images are transformed into Pytorch tensor object and normalized. 
            Both train and test sets are shuffled. The batch size of the loaders are the same as the size of class,
            which is 26. One example of a batch of images with their ground truth is shown below. 
        </p>

        </p>
        <img src="../img/portfolio/ws-example-batch.jpg" style="width:40%">

        <p>
            The neural network used during training has two 2D convolution layers and three hidden layers.
            At the beginning, a 2d max pooling is applied to the inputs. All layers are followed by rectified linear units (ReLU) 
            as their activation functions.
        </p>
        <img src="../img/portfolio/ws-nn-architecture.png" style="width:60%">
        
        <p>
            The overall accuray of trained model on EMNIST letter dataset is around 93.3%. 
            Accuracies on each class for most classes are above 90%. The image below shows accuracies of model on each class.  
        </p>
        <img src="../img/portfolio/ws-test-acc.jpg" style="width:60%">
        
        <p>
            As seen from the image above, the exceptions are G, I, a L. It is reasonable that I and L 
            are easily confused, given that lower case i and l are very similar. As for G, this is likely to be
            confused with lowercase Q. The images below are lowercase letters I, L, Q, G from left to right. The model
            is able to correctly classify around 70% of these letters in the testing set.
        </p>
        <div class="container">
            <div clsas="row">
                <div class="col-lg-2 ">
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_i.png" style="width:80%" >  
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_l.png" style="width:80%" >  
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_q.png" style="width:80%" >  
                </div>
                <div class="col-lg-2 ">
                    <img src="../img/portfolio/emnist_g.png" style="width:80%" >  
                </div>
            </div>
        </div>

        <p>
            To test user inputs, at least three images are stored for each label. The overall accuracy is 96.3%. 
            Accuracies for each class is shown below. Notice that the test set has only 178 images and are all 
            written under the same condition, so the accuracies may be bias. 
        </p>
        <img src="../img/portfolio/ws-user-input-test.jpg"  style="width:60%" > 
        
        <p>
            Most classes are able to be classified. The images that are misclassified is likely due to 
            the shaking behavior while writing. Some of the lines are not correctly bounded or closed. 
            One example is the D below that is classified as P. 
        </p>
        <img src="../img/portfolio/ws-user-d.jpg"  style="width:10%" > 
        
        <hr class="para-break" data-content="  Computer Vision  " id="computer-vision">
        <p>
            At start of writing this project, tracking a pen is configured by filtering out a specific color range. 
            Here, a red colored pen is used. To make detecting the color range easier, the also uses 
            <a href="https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html" target="_blank" rel="noopener noreferrer">OpenCV background subtraction </a>
            function to overlay a grey mask on the steady background. 
            However, some false positives still persist. To filter out the noises, position of pen on screen is 
            defined as the averge of positions in five consecutive frames. As a result, tracking the pen is a little slow. 
            This sometimes cause the strokes to be unsmooth. 
        </p>
        <br>
        <p>
            To improvie performance, this project also uses 
            <a href="https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html" target="_blank" rel="noopener noreferrer"> Haar Cascade </a>
            train and detect functions provided by OpenCV to track a pen. 
            The pen used for tracking is bought from  
            <a href="https://yoobi.com/collections/pens/products/scented-gel-pens-6-pack-fruit" target="_blank" rel="noopener noreferrer">yoobi</a>.
            To ensure that the pen is easily classifiable, a distinctive portion of the pen is chosen as the object
            to detect. The tracked portion is bounded by a red box in the image shown on the right below. 
        </p>
        <div class="container">
            <div clsas="row">
                <div class="col-lg-2 ">
                </div>
                <div class="col-lg-3 ">
                    <img src="https://cdn.shopify.com/s/files/1/0445/6817/products/TARGET_INLINE_MARCH_2020-4410-CROPPED_1000x.jpg?v=1582241292" 
                    style="width:100%" >  
                </div>
                <div class="col-lg-5 ">
                    <img src="../img/portfolio/ws-tracked-pen.png" style="width:40%" >  
                </div>
            </div>
        </div>

        <p>
            The Haar Cascade model used in this project is trained using around 1,000 positive images and 500 negative images.
            The classifier is able to correctly detect the pen in front of the camera; however, it will occasionally misclassify 
            other objects (such as fingers or shadows of similar shape) as the pen. To solve this problem without having to 
            retrain the model with more images, the code assumes that only one object is detected at each frame and that 
            10 objects need to be detected before defining it as the pen. The code also bounds the size of detection between 30x30 and 80x80 
            pixels. With these conditions, small noises are neglected, but it also has the risk of not recognizing 
            the pen due to large detectio threshold. This problem is solved by increasing the frame rate. 
        </p>

        <p>
            <br>
            When processessing color frames from depth camera at a frame rate of 60 fps,
            the performance is better than running at 30 fps. However, more false detections are found in this case, 
            so track function adds in another condition that if the current detection is around twice width or 
            twice height pixels away from the previous detection, then this detection is considered incorrect. 
            The imges belows show computer vision tracking a pen using color filtering at 60fps, Haar Cascade model 
            at 30 fps, and 60 fps respectively.
        </p>
        
        <div class="container">
            <div clsas="row">
                <div class="col-lg-1 "></div>
                <div class="col-lg-3 ">
                    <img src="../img/portfolio/ws-track-color.gif" style="width:100%">  
                </div>
                <div class="col-lg-4 ">
                    <img src="../img/portfolio/ws-30fps.gif" style="width:70%" >  
                </div>
                <div class="col-lg-3">
                    <img src="../img/portfolio/ws-60fps.gif" style="width:100%" >  
                </div>
            </div>
        </div>

        <hr class="para-break" data-content="  Integration  " id="integration">
        <p>
            This project consists of two main classes: Tracker and Classifier. 
            Tracker is responsible for tracking a pen in front of a depth camera, and Classifier
            is responsible for classifying user inputs.
        </p>
        <img src="../img/portfolio/wc-simple-workflow.png" style="width:40%" >  
        <p>
            To allow proper linking between different tasks, both Tracker and Classifier needs to have access to 
            folders <i>user_images</i>, where writting images are stored. When user is done with writing, Tracker first croppes  
            the screen based on where the writing is. Then, it stores the cropped frame as an image into <i>user_images</i> and calls
            function classify from Classifier. This function takes the image file name as an input so that when 
            loading the data using UserImageDataSet loader, the correct image is extracted. When loading user inputs, images 
            are resized to 28x28 px to makes sure that writings of different sizes are also correctly processed. 
            Other transforms are the same as in training. Lastly, the transformed data is input into the pre-trained PyTorch model 
            to predict its class. 
        </p>

        <hr class="para-break" data-content="  Related Links  " id="related-links">
        <p>
            <i class="fa fa-github"></i>
            <a class="i_link" href="https://github.com/aonai/winter_project_handwriting_camera" target="_blank" rel="noopener noreferrer"> Source Code</a>
        </p>
    </body>
 

</html>  
